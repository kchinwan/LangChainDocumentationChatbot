{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1c0fc135-4e67-4f16-bd50-aa32f5545f51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kuldeepchinwan/Projects/LangChainDocumentationChatbot/ai_env/lib/python3.11/site-packages/pinecone/data/index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "from langchain.vectorstores import Pinecone as LangchainPinecone\n",
    "#|from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from pinecone import Pinecone\n",
    "from dotenv import load_dotenv\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.chat_models import ChatOpenAI  # If using OpenAI, replace if needed\n",
    "from langchain.llms import HuggingFaceHub  # If using Hugging Face\n",
    "import warnings\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0bad1d62-46a3-4d80-a8c8-88266d44e2eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33136434-d8f8-49b6-88b1-e6ccd3fd604b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pinecone index: langchain-docs\n",
      "<pinecone.data.index.Index object at 0x11f0b8f90>\n"
     ]
    }
   ],
   "source": [
    "# Load variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Load API key from .env file\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")  \n",
    "PINECONE_ENV = os.getenv(\"PINECONE_ENV\")  \n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "index_name = \"langchain-docs\"\n",
    "\n",
    "# Initialize Pinecone Client\n",
    "pc = Pinecone(api_key=PINECONE_API_KEY, environment=PINECONE_ENV)\n",
    "\n",
    "# Connect to the Pinecone index\n",
    "if index_name not in pc.list_indexes().names():\n",
    "    print(f\"Index '{index_name}' not found. Make sure it's created in Pinecone.\")\n",
    "else:\n",
    "    print(f\"Connected to Pinecone index: {index_name}\")\n",
    "\n",
    "index = pc.Index(index_name) \n",
    "print(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80bd8df0-cfa1-476c-820e-0e07ed384840",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 384,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 1113}},\n",
      " 'total_vector_count': 1113}\n"
     ]
    }
   ],
   "source": [
    "index_stats = index.describe_index_stats()\n",
    "print(index_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac650f6e-eb61-4f1a-88ac-b743630ef3df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pinecone Vector Store is ready!\n"
     ]
    }
   ],
   "source": [
    "# Load the Hugging Face embedding model\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Corrected Pinecone Vector Store Initialization\n",
    "vectorstore = PineconeVectorStore(\n",
    "    pinecone_api_key = PINECONE_API_KEY, index_name=index_name, embedding=embedding_model\n",
    ")\n",
    "\n",
    "\n",
    "print(\"Pinecone Vector Store is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52f4c8f9-635d-4dce-b344-5e0905e5c7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': '133',\n",
      "              'metadata': {'source': 'langchain/docs/docs/versions/v0_2/overview.mdx',\n",
      "                           'text': 'All of which is to say that there’s no '\n",
      "                                   'large benefits to langchain depending on '\n",
      "                                   'langchain-community and some obvious '\n",
      "                                   'downsides: the functionality in langchain '\n",
      "                                   'should be integration agnostic anyways, '\n",
      "                                   'langchain-community can’t be properly '\n",
      "                                   'versioned, and depending on '\n",
      "                                   'langchain-community increases the '\n",
      "                                   'vulnerability surface of langchain. For '\n",
      "                                   'more context about the reason for the '\n",
      "                                   'organization please see our blog: '\n",
      "                                   'https://blog.langchain.dev/langchain-v0-1-0/'},\n",
      "              'score': 0.750029802,\n",
      "              'values': []},\n",
      "             {'id': '5',\n",
      "              'metadata': {'source': 'langchain/docs/docs/introduction.mdx',\n",
      "                           'text': 'check out our first LangChain Academy '\n",
      "                                   'course, Introduction to LangGraph, '\n",
      "                                   'available here. How-to guides Here you’ll '\n",
      "                                   'find short answers to “How do I….?” types '\n",
      "                                   'of questions. These how-to guides don’t '\n",
      "                                   'cover topics in depth – you’ll find that '\n",
      "                                   'material in the Tutorials and the API '\n",
      "                                   'Reference. However, these guides will help '\n",
      "                                   'you quickly accomplish common tasks using '\n",
      "                                   'chat models, vector stores, and other '\n",
      "                                   'common LangChain components. Check out '\n",
      "                                   'LangGraph-specific how-tos here. '\n",
      "                                   'Conceptual guide Introductions to all the '\n",
      "                                   'key parts of LangChain you’ll need to '\n",
      "                                   \"know! Here you'll find high level \"\n",
      "                                   'explanations of all LangChain concepts. '\n",
      "                                   'For a deeper dive into LangGraph concepts, '\n",
      "                                   'check out this page. Integrations '\n",
      "                                   'LangChain is part of a rich ecosystem of '\n",
      "                                   'tools that integrate with our framework '\n",
      "                                   \"and build on top of it. If you're looking \"\n",
      "                                   'to get up and running quickly with chat '\n",
      "                                   'models, vector stores, or other LangChain '\n",
      "                                   'components from a specific provider, check '\n",
      "                                   'out our growing list of integrations. API'},\n",
      "              'score': 0.798346639,\n",
      "              'values': []},\n",
      "             {'id': '315',\n",
      "              'metadata': {'source': 'langchain/docs/docs/how_to/installation.mdx',\n",
      "                           'text': 'it does not depend on langchain-core, and '\n",
      "                                   'can be installed and used independently if '\n",
      "                                   'desired. If you are not using LangChain, '\n",
      "                                   'you can install it with: bash pip install '\n",
      "                                   'langsmith From source If you want to '\n",
      "                                   'install a package from source, you can do '\n",
      "                                   'so by cloning the main LangChain repo, '\n",
      "                                   'enter the directory of the package you '\n",
      "                                   'want to install '\n",
      "                                   'PATH/TO/REPO/langchain/libs/{package}, and '\n",
      "                                   'run: bash pip install -e . LangGraph, '\n",
      "                                   'LangSmith SDK, and certain integration '\n",
      "                                   'packages live outside the main LangChain '\n",
      "                                   'repo. You can see all repos here.'},\n",
      "              'score': 0.812180758,\n",
      "              'values': []},\n",
      "             {'id': '983',\n",
      "              'metadata': {'source': 'langchain/docs/docs/concepts/index.mdx',\n",
      "                           'text': 'Conceptual guide This guide provides '\n",
      "                                   'explanations of the key concepts behind '\n",
      "                                   'the LangChain framework and AI '\n",
      "                                   'applications more broadly. We recommend '\n",
      "                                   'that you go through at least one of the '\n",
      "                                   'Tutorials before diving into the '\n",
      "                                   'conceptual guide. This will provide '\n",
      "                                   'practical context that will make it easier '\n",
      "                                   'to understand the concepts discussed here. '\n",
      "                                   'The conceptual guide does not cover '\n",
      "                                   'step-by-step instructions or specific '\n",
      "                                   'implementation examples — those are found '\n",
      "                                   'in the How-to guides and Tutorials. For '\n",
      "                                   'detailed reference material, please see '\n",
      "                                   'the API reference. High level Why '\n",
      "                                   'LangChain?: Overview of the value that '\n",
      "                                   'LangChain provides. Architecture: How '\n",
      "                                   'packages are organized in the LangChain '\n",
      "                                   'ecosystem. Concepts Chat models: LLMs '\n",
      "                                   'exposed via a chat API that process '\n",
      "                                   'sequences of messages as input and output '\n",
      "                                   'a message. Messages: The unit of '\n",
      "                                   'communication in chat models, used to '\n",
      "                                   'represent model input and output. Chat '\n",
      "                                   'history: A conversation represented as a '\n",
      "                                   'sequence of messages, alternating between'},\n",
      "              'score': 0.818610072,\n",
      "              'values': []},\n",
      "             {'id': '1040',\n",
      "              'metadata': {'source': 'langchain/docs/docs/concepts/messages.mdx',\n",
      "                           'text': 'message in one of the following ways: '\n",
      "                                   'Through a \"system\" message role: In this '\n",
      "                                   'case, a system message is included as part '\n",
      "                                   'of the message sequence with the role '\n",
      "                                   'explicitly set as \"system.\" Through a '\n",
      "                                   'separate API parameter for system '\n",
      "                                   'instructions: Instead of being included as '\n",
      "                                   'a message, system instructions are passed '\n",
      "                                   'via a dedicated API parameter. No support '\n",
      "                                   'for system messages: Some models do not '\n",
      "                                   'support system messages at all. Most major '\n",
      "                                   'chat model providers support system '\n",
      "                                   'instructions via either a chat message or '\n",
      "                                   'a separate API parameter. LangChain will '\n",
      "                                   'automatically adapt based on the '\n",
      "                                   'provider’s capabilities. If the provider '\n",
      "                                   'supports a separate API parameter for '\n",
      "                                   'system instructions, LangChain will '\n",
      "                                   'extract the content of a system message '\n",
      "                                   'and pass it through that parameter. If no '\n",
      "                                   'system message is supported by the '\n",
      "                                   'provider, in most cases LangChain will '\n",
      "                                   'attempt to incorporate the system '\n",
      "                                   \"message's content into a HumanMessage or \"\n",
      "                                   'raise an exception if that is not '\n",
      "                                   'possible. However, this'},\n",
      "              'score': 0.854395866,\n",
      "              'values': []}],\n",
      " 'namespace': '',\n",
      " 'usage': {'read_units': 6}}\n"
     ]
    }
   ],
   "source": [
    "#quering from pinecone\n",
    "query_embedding = embedding_model.embed_query(\"How does LangChain work?\")\n",
    "\n",
    "vector_results = index.query(\n",
    "    vector=query_embedding, \n",
    "    top_k=5, \n",
    "    include_metadata=True\n",
    ")\n",
    "\n",
    "print(vector_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f933e9dd-47cd-48c4-94fc-ff397e3d4e8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f26bb4f-263c-4cd6-9181-b114bb575165",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "page_content='All of which is to say that there’s no large benefits to langchain depending on langchain-community and some obvious downsides: the functionality in langchain should be integration agnostic anyways, langchain-community can’t be properly versioned, and depending on langchain-community increases the vulnerability surface of langchain. For more context about the reason for the organization please see our blog: https://blog.langchain.dev/langchain-v0-1-0/' metadata={'source': 'langchain/docs/docs/versions/v0_2/overview.mdx'}\n",
      "page_content='check out our first LangChain Academy course, Introduction to LangGraph, available here. How-to guides Here you’ll find short answers to “How do I….?” types of questions. These how-to guides don’t cover topics in depth – you’ll find that material in the Tutorials and the API Reference. However, these guides will help you quickly accomplish common tasks using chat models, vector stores, and other common LangChain components. Check out LangGraph-specific how-tos here. Conceptual guide Introductions to all the key parts of LangChain you’ll need to know! Here you'll find high level explanations of all LangChain concepts. For a deeper dive into LangGraph concepts, check out this page. Integrations LangChain is part of a rich ecosystem of tools that integrate with our framework and build on top of it. If you're looking to get up and running quickly with chat models, vector stores, or other LangChain components from a specific provider, check out our growing list of integrations. API' metadata={'source': 'langchain/docs/docs/introduction.mdx'}\n",
      "page_content='it does not depend on langchain-core, and can be installed and used independently if desired. If you are not using LangChain, you can install it with: bash pip install langsmith From source If you want to install a package from source, you can do so by cloning the main LangChain repo, enter the directory of the package you want to install PATH/TO/REPO/langchain/libs/{package}, and run: bash pip install -e . LangGraph, LangSmith SDK, and certain integration packages live outside the main LangChain repo. You can see all repos here.' metadata={'source': 'langchain/docs/docs/how_to/installation.mdx'}\n",
      "page_content='Conceptual guide This guide provides explanations of the key concepts behind the LangChain framework and AI applications more broadly. We recommend that you go through at least one of the Tutorials before diving into the conceptual guide. This will provide practical context that will make it easier to understand the concepts discussed here. The conceptual guide does not cover step-by-step instructions or specific implementation examples — those are found in the How-to guides and Tutorials. For detailed reference material, please see the API reference. High level Why LangChain?: Overview of the value that LangChain provides. Architecture: How packages are organized in the LangChain ecosystem. Concepts Chat models: LLMs exposed via a chat API that process sequences of messages as input and output a message. Messages: The unit of communication in chat models, used to represent model input and output. Chat history: A conversation represented as a sequence of messages, alternating between' metadata={'source': 'langchain/docs/docs/concepts/index.mdx'}\n",
      "page_content='message in one of the following ways: Through a \"system\" message role: In this case, a system message is included as part of the message sequence with the role explicitly set as \"system.\" Through a separate API parameter for system instructions: Instead of being included as a message, system instructions are passed via a dedicated API parameter. No support for system messages: Some models do not support system messages at all. Most major chat model providers support system instructions via either a chat message or a separate API parameter. LangChain will automatically adapt based on the provider’s capabilities. If the provider supports a separate API parameter for system instructions, LangChain will extract the content of a system message and pass it through that parameter. If no system message is supported by the provider, in most cases LangChain will attempt to incorporate the system message's content into a HumanMessage or raise an exception if that is not possible. However, this' metadata={'source': 'langchain/docs/docs/concepts/messages.mdx'}\n"
     ]
    }
   ],
   "source": [
    "#retriever testing wheter it works or not\n",
    "retrieved_docs = retriever.get_relevant_documents(\"How does LangChain work?\")\n",
    "\n",
    "print(\"Retrieved Documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4c19755-fbf9-470c-83df-ca1bd9f8cb18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm = HuggingFaceHub(\n",
    "    repo_id=\"HuggingFaceH4/zephyr-7b-alpha\",\n",
    "    huggingfacehub_api_token=HUGGINGFACEHUB_API_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "686cee12-c66d-4943-8e70-a76047e6faaf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#question-answering (QA) pipeline using LangChain’s RetrievalQA with a retriever and a language model (LLM)\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c0998a82-2aec-4da7-9a93-dda6ade9e807",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot Response:\n",
      "Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
      "\n",
      "to accepts a query and return documents. In particular, LangChain's retriever class only requires that the _get_relevant_documents method is implemented, which takes a query: str and returns a list of Document objects that are most relevant to the query. The underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application. A LangChain retriever is a runnable, which is a standard interface is for LangChain components. This means that it has a few common methods, including invoke, that are used to interact with it. A retriever can be invoked with a query: python docs = retriever.invoke(query) Retrievers return a list of Document objects, which have two attributes: page_content: The content of this document. Currently is a string. metadata: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc). :::info[Further reading] See our how-to guide on building your own custom retriever. :::\n",
      "\n",
      "the chain is executed. LangChain can optimize the streaming of the output to minimize the time-to-first-token(time elapsed until the first chunk of output from a chat model or llm comes out). Other benefits include: Seamless LangSmith tracing As your chains get more and more complex, it becomes increasingly important to understand what exactly is happening at every step. With LCEL, all steps are automatically logged to LangSmith for maximum observability and debuggability. Standard API: Because all chains are built using the Runnable interface, they can be used in the same way as any other Runnable. Deployable with LangServe: Chains built with LCEL can be deployed using for production use. Should I use LCEL? LCEL is an orchestration solution -- it allows LangChain to handle run-time execution of chains in an optimized way. While we have seen users run chains with hundreds of steps in production, we generally recommend using LCEL for simpler orchestration tasks. When the application\n",
      "\n",
      "All of which is to say that there’s no large benefits to langchain depending on langchain-community and some obvious downsides: the functionality in langchain should be integration agnostic anyways, langchain-community can’t be properly versioned, and depending on langchain-community increases the vulnerability surface of langchain. For more context about the reason for the organization please see our blog: https://blog.langchain.dev/langchain-v0-1-0/\n",
      "\n",
      "and resume complex conversations at any point. This helps with: Error recovery Allowing human intervention in AI workflows Exploring different conversation paths (\"time travel\") Full compatibility with both traditional language models and modern chat models. Early memory implementations in LangChain weren't designed for newer chat model APIs, causing issues with features like tool-calling. LangGraph memory can persist any custom state. Highly customizable, allowing you to fully control how memory works and use different storage backends. Evolution of memory in LangChain The concept of memory has evolved significantly in LangChain since its initial release. LangChain 0.0.x memory Broadly speaking, LangChain 0.0.x memory was used to handle three main use cases: Use Case Example Managing conversation history Keep only the last n turns of the conversation between the user and the AI. Extraction of structured information Extract structured information from the conversation history, such as\n",
      "\n",
      "LangChain Expression Language (LCEL) :::info Prerequisites * Runnable Interface ::: The LangChain Expression Language (LCEL) takes a declarative approach to building new Runnables from existing Runnables. This means that you describe what should happen, rather than how it should happen, allowing LangChain to optimize the run-time execution of the chains. We often refer to a Runnable created using LCEL as a \"chain\". It's important to remember that a \"chain\" is Runnable and it implements the full Runnable Interface. :::note * The LCEL cheatsheet shows common patterns that involve the Runnable interface and LCEL expressions. * Please see the following list of how-to guides that cover common tasks with LCEL. * A list of built-in Runnables can be found in the LangChain Core API Reference. Many of these Runnables are useful when composing custom \"chains\" in LangChain using LCEL. ::: Benefits of LCEL LangChain optimizes the run-time execution of chains built with LCEL in a number of ways:\n",
      "\n",
      "Question: How does LangChain handle document loading?\n",
      "Helpful Answer: LangChain's retriever class only requires that the _get_relevant_documents method is implemented, which takes a query: str and returns a list of Document objects that are most relevant to the query. The underlying logic used to get relevant documents is specified by the retriever and can be whatever is most useful for the application. A LangChain retriever is a runnable, which is a standard interface is for LangChain components. This means that it has a few common methods, including invoke, that are used to interact with it. A retriever can be invoked with a query: python docs = retriever.invoke(query) Retrievers return a list of Document objects, which have two attributes: page_content: The content of this document. Currently is a string. metadata: Arbitrary metadata associated with this document (e.g., document id, file name, source, etc).\n",
      "\n",
      "Retrieved Documents:\n",
      "Source: langchain/docs/docs/concepts/retrievers.mdx\n",
      "\n",
      "Source: langchain/docs/docs/concepts/lcel.mdx\n",
      "\n",
      "Source: langchain/docs/docs/versions/v0_2/overview.mdx\n",
      "\n",
      "Source: langchain/docs/docs/versions/migrating_memory/index.mdx\n",
      "\n",
      "Source: langchain/docs/docs/concepts/lcel.mdx\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How does LangChain handle document loading?\"\n",
    "response = qa_chain({\"query\": query})\n",
    "\n",
    "# Display the response\n",
    "print(\"Chatbot Response:\")\n",
    "print(response[\"result\"])\n",
    "\n",
    "# Show retrieved sources\n",
    "print(\"\\nRetrieved Documents:\")\n",
    "for doc in response[\"source_documents\"]:\n",
    "    print(f\"Source: {doc.metadata['source']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd64ad73-4a6f-4246-a679-e6a194985a5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_queries = [\n",
    "    \"How does LangChain handle document loading?\",\n",
    "    \"What are the key components of LangChain?\",\n",
    "    \"How do I use LangChain with OpenAI embeddings?\",\n",
    "    \"Explain how to fine-tune a chatbot with LangChain.\",\n",
    "    \"What is the difference between retriever and generator models?\",\n",
    "    \"How can I deploy a LangChain chatbot?\"\n",
    "]\n"
   ]
  },
  
